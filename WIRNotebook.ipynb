{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "WIRNotebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiseBtETqfIC",
        "colab_type": "text"
      },
      "source": [
        "# Configuration\n",
        "Install and import services."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TzTZcDFUaEW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "adbe38d1-d8e3-494f-a727-15d15fabe034"
      },
      "source": [
        "# General purpose modules\n",
        "import time\n",
        "import sys\n",
        "import os\n",
        "\n",
        "#Read csv files adn plots\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "import ast\n",
        "\n",
        "#Retrieval from twitter\n",
        "import tweepy\n",
        "from getpass import getpass\n",
        "\n",
        "#Tweet preprocessing tasks\n",
        "!pip install emoji\n",
        "!pip install pyspellchecker\n",
        "import re\n",
        "import emoji\n",
        "from spellchecker import SpellChecker\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "#random numbers\n",
        "from random import randint\n",
        "\n",
        "# Progress bar utility\n",
        "def initialize_progress_bar():\n",
        "  # setup progressbar\n",
        "  toolbar_width = 40\n",
        "  sys.stdout.write(\"[%s]\" % (\" \" * toolbar_width))\n",
        "  sys.stdout.flush()\n",
        "  sys.stdout.write(\"\\b\" * (toolbar_width+1)) # return to start of line, after '['\n",
        "\n",
        "def manage_progress_bar(j):\n",
        "  if (j == 0): sys.stdout.write(\" \" * 4)\n",
        "  j += 1\n",
        "  n = 5\n",
        "  if (len(str(j*100//40)) < 2): n = 4\n",
        "  sys.stdout.write(\"\\b\" * n)\n",
        "  sys.stdout.write(\"- %s\" % (str(j*100//40)))\n",
        "  sys.stdout.write(\"%]\")\n",
        "  sys.stdout.flush()\n",
        "  if (j == 40): sys.stdout.write(\"\\n\") # end the progress bar\n",
        "  return j\n",
        "\n",
        "#Access to Google drive file system\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My\\ Drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.5.4)\n",
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.6/dist-packages (0.5.4)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sloux1o0d_P",
        "colab_type": "text"
      },
      "source": [
        "## Load files in primary storage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzLjPiG9UaEd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "99a4bc0d-19ba-4758-976f-2a9bdcde91d3"
      },
      "source": [
        "kbase = pd.read_csv('Datasets/WIR_P12/dbpedia-classes/DBP_wiki_data_notext_nocomma.csv')\n",
        "kbase.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'l1', 'l2', 'l3', 'wiki_name'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alIzxZMoUaEx",
        "colab_type": "text"
      },
      "source": [
        "## Knowledge Base Distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhOf1IPxLljE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_occurrences(level):\n",
        "\n",
        "    categories = kbase[level]\n",
        "\n",
        "    category_info_dict = {}\n",
        "\n",
        "    for cat in categories:\n",
        "        if cat in category_info_dict:\n",
        "            category_info_dict[cat] += 1\n",
        "        else:\n",
        "            category_info_dict[cat] = 0\n",
        "    return category_info_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hfa9bHv9UaE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "def plot_kb_distr(level):\n",
        "\n",
        "    category_info_dict = count_occurrences(level)\n",
        "\n",
        "    labels = list(category_info_dict.keys())\n",
        "    values = list(category_info_dict.values())\n",
        "\n",
        "    plt.figure(figsize=(20,len(values)))\n",
        "    plt.title('Total length of categories')\n",
        "    \n",
        "    cmap = {'l1': 'r', 'l2':'g', 'l3': 'b'}\n",
        "\n",
        "    for i, cat in enumerate(category_info_dict):\n",
        "        plt.barh(labels, values, color=cmap[level])\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    sorted_categories = sorted(category_info_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    if level == 'l2':\n",
        "        print('TOP 23 CATEGORIES IN THE KB:')\n",
        "        for cat, value in sorted_categories[:23]:\n",
        "            print('{}: {} pages'.format(cat, value))\n",
        "            \n",
        "# plot_kb_distr('l1')\n",
        "# plot_kb_distr('l2')\n",
        "# plot_kb_distr('l3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml8m_m2rVK9S",
        "colab_type": "text"
      },
      "source": [
        "# Tweets dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3HzNoNzVeIH",
        "colab_type": "text"
      },
      "source": [
        "## Authentication via Twitter App\n",
        "1. Provide the keys of your twitter app.\n",
        "2. Establish an authenticated session with the Standard Twitter API through the tweepy library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Sbqfc4RVUds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# consumer_key = getpass('Enter consumer_key here')\n",
        "# consumer_secret = getpass('Enter consumer_secret here')\n",
        "# access_token = getpass('Enter access_token here')\n",
        "# access_token_secret = getpass('Enter access_token_secret here')\n",
        "\n",
        "# auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "# auth.set_access_token(access_token, access_token_secret)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXxL2TFDVgDi",
        "colab_type": "text"
      },
      "source": [
        "## Collect Tweets\n",
        "Collect tweets through the Twitter session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A79Qt0vVWdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def query_twitter(query):\n",
        "#     api = tweepy.API(auth)\n",
        "#     max_tweets = 5\n",
        "#     public_tweets = [status for status in tweepy.Cursor(api.search, q=query, lang='en', tweet_mode='extended').items(max_tweets)]\n",
        "#     i = 0\n",
        "\n",
        "#     for tweet in public_tweets:\n",
        "#         print(str(i) + ': ' + tweet.full_text)\n",
        "#         dictionary['tweets'].append(tweet.full_text)\n",
        "#         print('-------------------------------------------------------------------')\n",
        "#         i = i+1\n",
        "\n",
        "# #Examples of queries submitted to the twitter API\n",
        "# dictionary = {'tweets':[]}\n",
        "# query_twitter('National Geographic')\n",
        "# query_twitter('People Magazine')\n",
        "# query_twitter('Family Circle')\n",
        "# query_twitter('Game Informer Magazine')\n",
        "\n",
        "# #Create a Pandas dataframe and write it on a .csv file\n",
        "# dataframe = pd.DataFrame(dictionary)\n",
        "# dataframe.to_csv('tweets/periodical_literature.csv')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFgw5c2Gwuxg",
        "colab_type": "text"
      },
      "source": [
        "# Implementation of the system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4CeVKpmoNF-",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess tweet\n",
        "* method to preprocess the tweet:\n",
        "    * input: raw tweet\n",
        "    * output: list of tokens extracted from the raw tweet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIrwfmtjvrNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tweet_preprocessing(tweet):\n",
        "  #remove RT\n",
        "  tweet = re.sub(r'RT', '', tweet)\n",
        "  #remove urls from the tweet\n",
        "  tweet = re.sub(r'http\\S+', '', tweet)\n",
        "  #remove ats, hashtags and emails\n",
        "  tweet = re.sub(r'[^\\s]+@[^\\s]+', '', tweet)\n",
        "  tweet = re.sub(r'@', '', tweet)\n",
        "  tweet = re.sub(r'#', '', tweet)\n",
        "  #remove emojis, symbols and flags\n",
        "  tweet = emoji.get_emoji_regexp().sub(u'', tweet)\n",
        "  #tokenize tweet\n",
        "  tweet = word_tokenize(tweet)\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGlmJ7Svpww_",
        "colab_type": "text"
      },
      "source": [
        "## Build prefix map\n",
        "* This code snippet creates a prefix map over the entries of the *DBP_wiki_data_notext_nocomma.csv* knowledge base.\n",
        "* The prefix map is kept in main memory under the *prefix_map* variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-q2JQeJ5p15A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bebd098d-4d84-4a9a-e05e-6de65459535d"
      },
      "source": [
        "# entity_list is simply a list of wiki_names like \"Cristiano_Ronaldo\"\n",
        "def prefix_map_add_batch(entity_list):\n",
        "  size = len(entity_list)\n",
        "  if (size>40):\n",
        "    initialize_progress_bar()\n",
        "  i_prog = j_prog = 0  # progress bar variables\n",
        "\n",
        "  for entry in entity_list:  # entry i.e. each wiki_name\n",
        "    i_prog += 1\n",
        "    prefix = ''\n",
        "    for token in word_tokenize(entry.replace('_', ' ')):\n",
        "      prefix = (prefix + ' ' + token).strip()  # update prefix\n",
        "      \n",
        "      kb_node = None\n",
        "      stop = False\n",
        "      if prefix == entry.replace('_', ' '):  # if prefix == entry\n",
        "        kb_node = entry  # set the kb_node\n",
        "        stop = True\n",
        "\n",
        "      try:\n",
        "        # check if the prefix is already in the map\n",
        "        found = False\n",
        "        for i in range(len(prefix_map[hash(prefix)])):\n",
        "          # if it is already contained\n",
        "          if prefix == prefix_map[hash(prefix)][i][0]:\n",
        "            found = True\n",
        "            # if we are not done with the current entry, set stop=False for the map entry\n",
        "            # otherwise, leave it unchanged\n",
        "            if not stop:\n",
        "              prefix_map[hash(prefix)][i][2] = False\n",
        "            else:\n",
        "              prefix_map[hash(prefix)][i][1] = kb_node\n",
        "            break\n",
        "        # if it is not, append it\n",
        "        if not found:\n",
        "          prefix_map[hash(prefix)].append([prefix, kb_node, stop])\n",
        "      except KeyError:\n",
        "        prefix_map[hash(prefix)] = [[prefix, kb_node, stop]]\n",
        "    # Managing the progress bar\n",
        "    if (size > 40 and i_prog % (size//40) == 0):\n",
        "      j_prog = manage_progress_bar(j_prog)\n",
        "\n",
        "prefix_map = {}\n",
        "prefix_map_add_batch(kbase['wiki_name'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[                                        ]\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b    \b\b\b\b- 2%]\b\b\b\b- 5%]\b\b\b\b- 7%]\b\b\b\b\b- 10%]\b\b\b\b\b- 12%]\b\b\b\b\b- 15%]\b\b\b\b\b- 17%]\b\b\b\b\b- 20%]\b\b\b\b\b- 22%]\b\b\b\b\b- 25%]\b\b\b\b\b- 27%]\b\b\b\b\b- 30%]\b\b\b\b\b- 32%]\b\b\b\b\b- 35%]\b\b\b\b\b- 37%]\b\b\b\b\b- 40%]\b\b\b\b\b- 42%]\b\b\b\b\b- 45%]\b\b\b\b\b- 47%]\b\b\b\b\b- 50%]\b\b\b\b\b- 52%]\b\b\b\b\b- 55%]\b\b\b\b\b- 57%]\b\b\b\b\b- 60%]\b\b\b\b\b- 62%]\b\b\b\b\b- 65%]\b\b\b\b\b- 67%]\b\b\b\b\b- 70%]\b\b\b\b\b- 72%]\b\b\b\b\b- 75%]\b\b\b\b\b- 77%]\b\b\b\b\b- 80%]\b\b\b\b\b- 82%]\b\b\b\b\b- 85%]\b\b\b\b\b- 87%]\b\b\b\b\b- 90%]\b\b\b\b\b- 92%]\b\b\b\b\b- 95%]\b\b\b\b\b- 97%]\b\b\b\b\b- 100%]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKT8vKbuwMiE",
        "colab_type": "text"
      },
      "source": [
        "## Build the Inverted Index\n",
        "Create an inverted index that, given a *wiki_name*, returns the corresponding lineage [l1,l2,l3]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RreIfR_JXycx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9114164b-10aa-4e48-dc59-e267c53cc578"
      },
      "source": [
        "initialize_progress_bar()\n",
        "\n",
        "inv_index = {}\n",
        "i = j = 0 # parameters for the progress bar\n",
        "for index, row in kbase.iterrows():\n",
        "  i += 1\n",
        "  inv_index[row['wiki_name']] = [row['l1'], row['l2'], row['l3']]\n",
        "\n",
        "  # Managing the progress bar\n",
        "  if (i % (len(kbase)//40) == 0):\n",
        "    j = manage_progress_bar(j)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[                                        ]\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b    \b\b\b\b- 2%]\b\b\b\b- 5%]\b\b\b\b- 7%]\b\b\b\b\b- 10%]\b\b\b\b\b- 12%]\b\b\b\b\b- 15%]\b\b\b\b\b- 17%]\b\b\b\b\b- 20%]\b\b\b\b\b- 22%]\b\b\b\b\b- 25%]\b\b\b\b\b- 27%]\b\b\b\b\b- 30%]\b\b\b\b\b- 32%]\b\b\b\b\b- 35%]\b\b\b\b\b- 37%]\b\b\b\b\b- 40%]\b\b\b\b\b- 42%]\b\b\b\b\b- 45%]\b\b\b\b\b- 47%]\b\b\b\b\b- 50%]\b\b\b\b\b- 52%]\b\b\b\b\b- 55%]\b\b\b\b\b- 57%]\b\b\b\b\b- 60%]\b\b\b\b\b- 62%]\b\b\b\b\b- 65%]\b\b\b\b\b- 67%]\b\b\b\b\b- 70%]\b\b\b\b\b- 72%]\b\b\b\b\b- 75%]\b\b\b\b\b- 77%]\b\b\b\b\b- 80%]\b\b\b\b\b- 82%]\b\b\b\b\b- 85%]\b\b\b\b\b- 87%]\b\b\b\b\b- 90%]\b\b\b\b\b- 92%]\b\b\b\b\b- 95%]\b\b\b\b\b- 97%]\b\b\b\b\b- 100%]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA0bKyC9FyTS",
        "colab_type": "text"
      },
      "source": [
        "## Extract Mentions\n",
        "* This cell contains a function that has the role to search formentions in a given list of tweets\n",
        "* This search exploits the prefix map, in order to search efficiently in the knowledge base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxWf3kfVF5BA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_mentions(tweets):\n",
        "  tweets_mentions = []\n",
        "  #scan all the tweets in the dataset\n",
        "  for tokens in tweets:\n",
        "    #list of mentions for the current tweet\n",
        "    mentions = []\n",
        "    for i in range(len(tokens)):\n",
        "      token = tokens[i]\n",
        "      #if the token is in the kb\n",
        "      try:\n",
        "        #True if you have found a mention\n",
        "        done = False\n",
        "        #scan consecutive tokens in the tweet\n",
        "        j = 1;\n",
        "        while (i+j) <= len(tokens):\n",
        "          #Keep track of the last kb node found\n",
        "          last_kb_node = None\n",
        "          #compute the hash for the selected tokens\n",
        "          prefix_map_list = prefix_map[hash(token)]\n",
        "          #scan the adjacency list of the HashMap\n",
        "          for prefix_map_token in prefix_map_list:\n",
        "            #take the entry that you need from the adjacency list\n",
        "            if prefix_map_token[0].lower() == token.lower():\n",
        "              #stop is true, return what you have\n",
        "              if prefix_map_token[2] == True:\n",
        "                last_kb_node = prefix_map_token[1]\n",
        "                mentions.append(last_kb_node)\n",
        "                done = True\n",
        "              #stop is false\n",
        "              else:\n",
        "                if prefix_map_token[1] != None:\n",
        "                    last_kb_node = prefix_map_token[1]\n",
        "                if i+j != len(tokens):\n",
        "                  token = token + ' ' + tokens[i+j]\n",
        "                  j += 1\n",
        "                else:\n",
        "                  j += 1\n",
        "          #Found a mention\n",
        "          if done == True:\n",
        "              next_token = True\n",
        "              break\n",
        "\n",
        "        if last_kb_node != None:\n",
        "            mentions.append(last_kb_node)\n",
        "      except KeyError:\n",
        "          pass\n",
        "    #Add the final mentions for this tweet to a global list\n",
        "    tweets_mentions.append(mentions)\n",
        "  return tweets_mentions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nekjT3fWIEnT",
        "colab_type": "text"
      },
      "source": [
        "## Classifying and Tagging the Tweet\n",
        "Dopo aver estratto le mentions di un tweet dalla prefix map, posso procedere alla classificazione.\n",
        "\n",
        "```\n",
        "# INPUT(mentions) // we refer to the mentions of the tweet to be classified\n",
        "# OUTPUT(class_l2)\n",
        "```\n",
        "\n",
        "For each mention (m1, n1, s1):\n",
        "* Starting at n1, we go up all lineages of n1, all the way to the root, and assign to each node in these lineages a score (currently set to be s1, the same score as that of n1).\n",
        "* If we find two partially overlapping lineages, the common nodes in the *kbase* should get an increased score. Start with s1+s2, later on we can refine this score.\n",
        "* Let C be the decreasingly ordered list of all scored nodes in all lineages. **Select from C all topic nodes**. Starting from C we can both produce a classification (pick the highest) and a tagging (pick all).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jioE-UH3l6Uw",
        "colab_type": "text"
      },
      "source": [
        "## Taxonomic score (struct)\n",
        "Produce a scoring data structure with:\n",
        "*   the shape of the taxonomic hierarchy\n",
        "*   an additional field **score**, as a placeholder of the score of that category.\n",
        "```\n",
        "## INPUT (./dbpedia-classes/taxonomy.txt)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0saatm-kkwp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_tax_score():\n",
        "  tax_file = open(\"Datasets/WIR_P12/dbpedia-classes/taxonomy.txt\", \"r\")\n",
        "  tax_score = {}\n",
        "\n",
        "  for line in tax_file:\n",
        "    if (not line[0]=='\\t'):\n",
        "      l1 = word_tokenize(line)[0]\n",
        "      tax_score[l1] = {}\n",
        "      tax_score[l1]['score'] = 0\n",
        "    else:\n",
        "      if (not line[1]=='\\t'):\n",
        "        l2 = word_tokenize(line)[0]\n",
        "        tax_score[l1][l2] = {}\n",
        "        tax_score[l1][l2]['score'] = 0\n",
        "      else:\n",
        "        l3 = word_tokenize(line)[0]\n",
        "        tax_score[l1][l2][l3] = {}\n",
        "        tax_score[l1][l2][l3]['score'] = 0\n",
        "  return tax_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERBwK8kHkfJK",
        "colab_type": "text"
      },
      "source": [
        "## Classifying and Tagging\n",
        "* Assign the scores on the lineages of the detected mentions.\n",
        "* I shall return the L2 categories in *score decreasing order*. Moreover, I can return the L1, L3 categories (when existing) as tags of the tweet.\n",
        "\n",
        "```\n",
        "# INPUT(mentions)\n",
        "# OUTPUT(categories, tags)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjHblrWtrHal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use only within the scope of classify_and_tag\n",
        "\n",
        "# Returns only the top scores categories from a list of \n",
        "# all categories sorted by score (reversed).\n",
        "def filter_classes(alist):\n",
        "  if (alist[0][1]==0):\n",
        "    return []\n",
        "  \n",
        "  top_list = []\n",
        "  top_list.append(alist[0][0])\n",
        "  max = alist.pop(0)[1]\n",
        "  for i in alist:\n",
        "    if (i[1]==max):\n",
        "      top_list.append(i[0])\n",
        "    else:\n",
        "      break\n",
        "  \n",
        "  return top_list\n",
        "\n",
        "# Returns only non-zero tags.\n",
        "def filter_tags(alist):\n",
        "  ret_list = []\n",
        "\n",
        "  for i in alist:\n",
        "    if (not i[1]==0):\n",
        "      ret_list.append(i[0])\n",
        "    else:\n",
        "      break\n",
        "  \n",
        "  return ret_list\n",
        "\n",
        "def classify_and_tag(tweet_mention): # the mentions of a single tweet\n",
        "  score = create_tax_score()\n",
        "  categories = []\n",
        "  tags = []\n",
        "  for mention in tweet_mention:\n",
        "    lineage = inv_index[mention]\n",
        "    l1 = lineage[0]\n",
        "    l2 = lineage[1]\n",
        "    l3 = lineage[2]\n",
        "    score[l1]['score'] += 1\n",
        "    score[l1][l2]['score'] += 1\n",
        "    if (not l3 == 'none'):\n",
        "      score[l1][l2][l3]['score'] += 1\n",
        "  for x in score.keys():\n",
        "    tags.append((x,score[x]['score']))\n",
        "    for y in score[x].keys():\n",
        "      if(y == 'score'): continue\n",
        "      categories.append((y,score[x][y]['score']))\n",
        "      tags.append((y,score[x][y]['score']))\n",
        "      for z in score[x][y].keys():\n",
        "        if(z == 'score'): continue\n",
        "        tags.append((z,score[x][y][z]['score']))\n",
        "  categories.sort(key=lambda x : x[1], reverse=True)\n",
        "  tags.sort(key=lambda x : x[1], reverse=True)\n",
        "  categories = filter_classes(categories)\n",
        "  tags = filter_tags(tags)\n",
        "\n",
        "  return (categories, tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKGCCEoSYCvc",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caeRimzuYHEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function that comutes precision, recall and f1 score\n",
        "def evaluation_report(pred, gt):\n",
        "  tp = 0\n",
        "  fp = 0\n",
        "  fn = 0\n",
        "\n",
        "  for i in range(len(pred)):\n",
        "    #build true positives, false positives, and false negatives counters\n",
        "    for prediction in pred[i]:\n",
        "      if prediction in gt[i]:\n",
        "        tp += 1\n",
        "      else:\n",
        "        fp += 1\n",
        "    for ground_truth in gt[i]:\n",
        "      if ground_truth not in pred[i]:\n",
        "          fn += 1\n",
        "  \n",
        "  #compute precision, recall, and f1 score\n",
        "  if tp != 0:\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    accuracy = tp / len(pred)\n",
        "  else:\n",
        "    return 0,0,0,0\n",
        "  return precision, recall, f1, accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc2nGsMU81Ox",
        "colab_type": "text"
      },
      "source": [
        "## Update of the knowledge base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKQmx2VQ86vF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Read the json file to memorize the taxonomy of the kb\n",
        "json_file = open('Datasets/WIR_P12/dbpedia-classes/taxonomy.json',)\n",
        "taxonomy = json.load(json_file)\n",
        "\n",
        "def search_prefix_map(token):\n",
        "    try:\n",
        "        token = token.replace('_', ' ')\n",
        "        #compute the hash for the selected tokens\n",
        "        prefix_map_list = prefix_map[hash(token)]\n",
        "        #scan the adjacency list of the HashMap\n",
        "        for prefix_map_token in prefix_map_list:\n",
        "            #take the entry that you need from the adjacency list\n",
        "            if prefix_map_token[0].lower() == token.lower():\n",
        "                return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# gt: List<List<Pair<Str,Str>>>, where \n",
        "# for_each p in l in gp | p = \"Cristiano_Ronaldo\", \"Athlete\", \n",
        "# I.E.: <\"Sample\", \"L2\">\n",
        "# Note: gp is the list of mentions (one for each tweet)\n",
        "def update_kb(gt):\n",
        "    global kbase\n",
        "\n",
        "    #create a unique dictionary from the groundtruth list\n",
        "    unique_gt = {}\n",
        "    for item in gt:\n",
        "        unique_gt.update(item)\n",
        "\n",
        "    for entity,category in unique_gt.items():\n",
        "        #check if the term is contained in the prefix_map and therefore in the kb\n",
        "        contained = search_prefix_map(entity)\n",
        "        if contained == False:\n",
        "            #take the l1 category from the current l2\n",
        "            root_category = taxonomy[category]\n",
        "            #append the new mention to the updated kb\n",
        "            kbase = kbase.append({'l1':root_category, 'l2':category, 'wiki_name':entity}, ignore_index=True)\n",
        "            prefix_map_add_batch([entity])\n",
        "            inv_index[entity] = [root_category, category, 'none']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STxUhf4TxzTP",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation of Mentions Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X806xWgazZXt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ea5ef655-9354-4b69-8529-ff1abf9995fc"
      },
      "source": [
        "#every data file has the name of the corresponding l2 category\n",
        "categories = []\n",
        "\n",
        "for filename in os.listdir('Datasets/WIR_P12/tweets/'):\n",
        "    categories.append(filename[:-4])\n",
        "\n",
        "pred = []\n",
        "gt_mentions = []\n",
        "\n",
        "for category in categories:\n",
        "    gt = []  # placeholder for the ground truth mentions for each tweet\n",
        "    \n",
        "    tweets = pd.read_csv('Datasets/WIR_P12/tweets/'+ category +'.csv')\n",
        "    #separate the ground truth from the tweet text\n",
        "    for mentions in tweets.mentions:\n",
        "        mentions = mentions.replace('\\'', '\"')\n",
        "        gt.append(json.loads(mentions))\n",
        "\n",
        "    #preprocess and extract mentions from tweets\n",
        "    for i in range(len(tweets.tweets)):\n",
        "        tweets.tweets[i] = tweet_preprocessing(tweets.tweets[i])\n",
        "    pred = pred + (extract_mentions(tweets.tweets))\n",
        "\n",
        "    #print the predictions and the ground truth\n",
        "    for i in range(len(tweets.tweets)):\n",
        "        gt_mentions.append(list(dict.fromkeys(gt[i])))\n",
        "\n",
        "    #Update the Knowledge base with the new groundtruth mentions\n",
        "    #Comment this line if, ater the evaluation of the mentions extraction, you want to evaluate unupdated classification\n",
        "    update_kb(gt)\n",
        "\n",
        "#print the evaluation scores\n",
        "precision, recall, f1, accuracy = evaluation_report(pred[2:], gt_mentions[2:])\n",
        "print('Precision: {}, Recall: {}, F1_score: {}'.format(precision, recall, f1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Precision: 0.9133858267716536, Recall: 0.5144124168514412, F1_score: 0.6581560283687944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVabDUeWKGjC",
        "colab_type": "text"
      },
      "source": [
        "## Random Distribution\n",
        "\n",
        "* this cell defines a function that makes a prediction on the category of a tweet\n",
        "* This prediction is based on a probability distribution taken from the topology of our knowledge base\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obGGz3PHKPec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def probabilistic_prediction(check = False, categories = kbase['l2']):\n",
        "    #this is the final prediction on the probability distribution of the kb\n",
        "    pred = None\n",
        "    categories = list(categories)\n",
        "    #we generate a random number from 1 to 100\n",
        "    prediction_number = randint(1,100)\n",
        "\n",
        "    #count the total number of categories in the kb\n",
        "    total_categories = 0\n",
        "    categories_counter = count_occurrences('l2')\n",
        "    for k,occurrences in categories_counter.items():\n",
        "        #we do this check in order to not search in the entire kb when categories == kbase['l2']\n",
        "        if check == True:\n",
        "            #if check is true, this means that we will search in a small list of categories\n",
        "            if k in categories:\n",
        "                total_categories += occurrences\n",
        "        else:\n",
        "            total_categories += occurrences\n",
        "    category_percentage = {}\n",
        "\n",
        "    #compute the probabilities with a similar method as above\n",
        "    for k,occurrences in categories_counter.items():\n",
        "        if check == True:\n",
        "            if k in categories:\n",
        "                category_percentage[k] = (occurrences / total_categories) * 100\n",
        "        else:\n",
        "            category_percentage[k] = (occurrences / total_categories) * 100\n",
        "\n",
        "    #create tresholds to predict the result\n",
        "    incremental_percentage = 0\n",
        "    for k,percentage in category_percentage.items():\n",
        "        incremental_percentage += percentage\n",
        "        if prediction_number <= incremental_percentage:\n",
        "            pred = k\n",
        "            break\n",
        "\n",
        "    return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-At212HvbEI",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation of Classification\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAql6zBlvlDK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4fd4977e-dfb6-4ec9-880d-3fa282bcc7a9"
      },
      "source": [
        "def classification_evaluation():\n",
        "    gt = []\n",
        "\n",
        "    for category in categories:\n",
        "        #Read_tweets from the dataset\n",
        "        tweets = pd.read_csv('Datasets/WIR_P12/tweets/'+ category +'.csv')\n",
        "        for i in range(len(tweets)):\n",
        "            gt.append([category])\n",
        "\n",
        "    classes = []\n",
        "    tags = []\n",
        "    for i in range(len(pred)):\n",
        "        x,y = classify_and_tag(pred[i])\n",
        "        classes.append(x)\n",
        "        tags.append(y)\n",
        "\n",
        "        #If nothing found predict something according to the knowledge base distribution\n",
        "        if classes[i] == []:\n",
        "            classes[i].append(probabilistic_prediction())\n",
        "\n",
        "        #If we have classes with the same score, choose one of them according to the knowledge base distribution\n",
        "        if len(classes[i]) > 1:\n",
        "            classes[i] = [probabilistic_prediction(True, classes[i])]\n",
        "\n",
        "    precision, recall, f1, accuracy = evaluation_report(classes,gt)\n",
        "    return precision, recall, f1, accuracy\n",
        "\n",
        "\n",
        "#When our system is undecided makes a probabilistic classification, so for evaluation purposes we repeat\n",
        "#the classification task 100 times and get the arithmetic mean of precision,recall,f1_score and accuracy\n",
        "precision, recall, f1, accuracy = 0, 0, 0, 0\n",
        "for i in range(100):\n",
        "    prec, rec, f, acc = classification_evaluation()\n",
        "    precision += prec\n",
        "    recall += rec\n",
        "    f1 += f\n",
        "    accuracy += acc\n",
        "precision = precision / 100\n",
        "recall = recall / 100\n",
        "f1 = f1 / 100\n",
        "accuracy = accuracy / 100\n",
        "\n",
        "print('Precision: {}, Recall: {}, F1_score: {}, Accuracy: {}'.format(precision, recall, f1, accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision: 0.9133858267716536, Recall: 0.5144124168514412, F1_score: 0.6581560283687944, Accuracy: 1.2747252747252746\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOK50VI978Il",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6721fd01-ee00-4941-a598-ddeae692d9a3"
      },
      "source": [
        "categories = []\n",
        "\n",
        "for filename in os.listdir('Datasets/WIR_P12/tweets/'):\n",
        "    categories.append(filename[:-4])\n",
        "\n",
        "i = 0\n",
        "for category in categories:\n",
        "    df = pd.read_csv('Datasets/WIR_P12/tweets/'+ category +'.csv')\n",
        "    for index, row in df.iterrows():\n",
        "      i += 1\n",
        "      print(i, \"Tweet: {\" + row['tweets'][:40] + \"... }\", \"Mentions:\", row['mentions'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 Tweet: {RT @JoeBiden: Donald Trump wants to styl... } Mentions: {'Donald_Trump':'Politician'}\n",
            "2 Tweet: {RT @TimHannan: Donald Trump is the bigge... } Mentions: {'Donald_Trump':'Politician'}\n",
            "3 Tweet: {RT @AlanBixter: \"Boris Johnson, Donald T... } Mentions: {'Boris_Johnson':'Politician','Donald_Trump':'Politician','Vladimir_Putin':'Politician','Jair_Bolsonaro':'Politician'}\n",
            "4 Tweet: {RT @kylegriffin1: ABC News: \"How would y... } Mentions: {'Donald_Trump':'Politician','Vladimir_Putin':'Politician'}\n",
            "5 Tweet: {@RimSarah @LibyaLiberty It made me reali... } Mentions: {'Silvio_Berlusconi':'Politician', 'Silvio_Berlusconi':'Politician'}\n",
            "6 Tweet: {@InstaFeminista @TNubian2 it was more a ... } Mentions: {'Silvio_Berlusconi':'Politician'}\n",
            "7 Tweet: {@celliottability @MackenzieHealth @fordn... } Mentions: {'Mario_Cortellucci':'Politician','Silvio_Berlusconi':'Politician', 'Giorgia_Meloni':'Politician', 'Matteo_Salvini':'Politician'}\n",
            "8 Tweet: {RT @PeteOlympian: @JimMFelton @SoozUK Bo... } Mentions: {'Boris_Johnson':'Politician'}\n",
            "9 Tweet: {RT @ThePoke: Boris Johnson’s official pl... } Mentions: {'Boris_Johnson':'Politician'}\n",
            "10 Tweet: {RT @Keir_Starmer: Child poverty rates ar... } Mentions: {'Boris_Johnson':'Politician'}\n",
            "11 Tweet: {RT @friendsofrss: Interestingly, the MoU... } Mentions: {'Rahul_Gandhi':'Person'}\n",
            "12 Tweet: {@SpokespersonCHN Xi Jinping is the unele... } Mentions: {'Xi_Jinping':'Person','Narendra_Modi':'Person','China':'Settlement','India':'Settlement'}\n",
            "13 Tweet: {Larry Page.- #quote #image https://t.co/... } Mentions: {'Larry_Page':'Person'}\n",
            "14 Tweet: {RT @Julez_Norton: Meet the Kitty Hawk Fl... } Mentions: {'Kitty_Hawk_Flyer':'Engine','Larry_Page':'Person'}\n",
            "15 Tweet: {@CAFODSchools @AllSaintsChap @ASCHSSheff... } Mentions: {'Pope_Francis':'Person','Sierra_Leone':'Settlement'}\n",
            "16 Tweet: {RT @OpusDeiUK: In his 17 June General Au... } Mentions: {'Pope_francis':'Person','Moses':'Person'}\n",
            "17 Tweet: {RT @VaticanIHD: Pope encourages #seafare... } Mentions: {'Pope_Francis':'Person','Covid-19':'NaturalEvent','Vatican':'Settlement'}\n",
            "18 Tweet: {RT @srivatsayb: Jawaharlal Nehru was a V... } Mentions: {'Jawaharlal_Nehru':'Person','Lal_Bahadur':'Person','Indira_Gandhi':'Person'}\n",
            "19 Tweet: {Always deliver more than expected.- Larr... } Mentions: {'Larry_Page':'Person'}\n",
            "20 Tweet: {@SpokespersonCHN Xi Jinping is the unele... } Mentions: {'Xi_Jinping':'Person','Narendra_Modi':'Person','China':'Settlement','India':'Settlement'}\n",
            "21 Tweet: {Visitors wearing masks are in Burj Khali... } Mentions: {'Burj_Khalifa':'Building'}\n",
            "22 Tweet: {RT @fbotha1: Burj Khalifa, tallest build... } Mentions: {'Burj_Khalifa':'Building'}\n",
            "23 Tweet: {At 8pm on June 14, 2020, the 17th World ... } Mentions: {'Shanghai':'Settlement','Oriental_Pearl_TV_Tower':'Building'}\n",
            "24 Tweet: {@FuatKircaali Expect Trump Tower Shangha... } Mentions: {'Trump Tower':'Building','Shanghai':'Settlement'}\n",
            "25 Tweet: {@lily_2387 i met Rebecca 17 years ago, i... } Mentions: {'Taiwan_Taipei':'Settlement','Taipei_101':'Building'}\n",
            "26 Tweet: {#Taipei 101 is Always beautiful 💗 https:... } Mentions: {'Taipei_101':'Building'}\n",
            "27 Tweet: {Wait!! So Xuan’s IG is Taipei’s 101 towe... } Mentions: {'Taipei_101':'Building'}\n",
            "28 Tweet: {[update] WCX IG story 💕 It’s Taipei’s 10... } Mentions: {'Taipei_101':'Building'}\n",
            "29 Tweet: {This has a chance given the resolution's... } Mentions: {'US_Capitol_Building':'Building','United_States':'Settlement'}\n",
            "30 Tweet: {@arranqh The most relevant I could sugge... } Mentions: {'Empire_State_Building':'Building','Chrysler_Building':'Building','Rockerfeller_Centre':'Building'}\n",
            "31 Tweet: {RT @BBCMOTD: For the first time in his c... } Mentions: {'Cristiano_Ronaldo':'Athlete'}\n",
            "32 Tweet: {RT @goal: Another one of those days for ... } Mentions: {'Cristiano_Ronaldo':'Athlete'}\n",
            "33 Tweet: {RT @LjsGoat: LeBron James really dropped... } Mentions: {'LeBron_James':'Athlete'}\n",
            "34 Tweet: {I love that phrase: \" Only Lionel Messi\"... } Mentions: {'Lionel_Messi':'Athlete'}\n",
            "35 Tweet: {RT @FaIseNueve: Reminder that Vinicius J... } Mentions: {'Vinicius_Junior':'Athlete', 'Lionel_Messi':'Athlete', 'Cristiano_Ronaldo':'Athlete', 'El_Clasico':'SportsEvent'}\n",
            "36 Tweet: {@tennisplayer651 @BetoCrypto @usopen Nad... } Mentions: {'Rafael_Nadal':'Athlete', 'United_States_of_America':'Settlement', 'Roger_Federer':'Athlete'}\n",
            "37 Tweet: {@_HelenDale I read an article by Roger A... } Mentions: {'Roger_Alton':'Person', 'Roger_Federer':'Athlete', 'Usain_Bolt':'Athlete'}\n",
            "38 Tweet: {Novak Djokovic: 'Nothing would surprise ... } Mentions: {'Novak_Djokovic':'Athlete', 'Roger_Federer':'Athlete'}\n",
            "39 Tweet: { \"Always wanted to tell you that there i... } Mentions: {'Roger_Federer':'Athlete'}\n",
            "40 Tweet: {Cheap Workers of Jesus: Roger Federer, S... } Mentions: {'Roger_Federer':'Athlete', 'Singampunari_Bala':'Athlete', 'Justin_Trudeau':'Athlete', 'Alia_Bhatt':'Athlete'}\n",
            "41 Tweet: {RT @DrewPavlou: Between my Amnesty Inter... } Mentions: {'Amnesty_International':'Organisation'}\n",
            "42 Tweet: {#ThrowbackThursday with a twist #shoutou... } Mentions: {'Amnesty_International':'Organisation','Justin_Mazzola':'Person'}\n",
            "43 Tweet: {@aruproytweets Since When Fitch became A... } Mentions: {'Amnesty_International':'Organisation'}\n",
            "44 Tweet: {RT @k_colonialism: In 2002, you could fe... } Mentions: {'FAO':'Organisation'}\n",
            "45 Tweet: {RT @FAONigeria: On #DesertificationAndDr... } Mentions: {'FAO':'Organisation'}\n",
            "46 Tweet: {@gri_adam @francediplo @ONU_fr Shut up g... } Mentions: {'ONU':'Organisation'}\n",
            "47 Tweet: {Trace the journey of #weightlifting in P... } Mentions: {'1896_Olympics':'SocietalEvent'}\n",
            "48 Tweet: {@suzydymna @realDonaldTrump A major even... } Mentions: {'Russian_Revolution':'SocietalEvent','Bloody_Sunday':'SocietalEvent'}\n",
            "49 Tweet: {Today you should go out with the boys wi... } Mentions: {'Russian_Revolution':'SocietalEvent'}\n",
            "50 Tweet: {@EuropeanFreedo1 @samisdat_info They als... } Mentions: {'Russian_Revolution':'SocietalEvent','Imperial_Russia':'Settlement'}\n",
            "51 Tweet: {RT @tensalad: wayv having a new cat is m... } Mentions: {'Cat':'Animal'}\n",
            "52 Tweet: {RT @CatherineC214: Cat and Mouse - Episo... } Mentions: {'Cat':'Animal','Mouse':'Animal'}\n",
            "53 Tweet: {soon after it’s done drinking, i heard t... } Mentions: {'Cat':'Animal'}\n",
            "54 Tweet: {RT @Beomgyusoulmate: 3 OBVIOUSLY, LIKE T... } Mentions: {'Dog':'Animal'}\n",
            "55 Tweet: {RT @angie_karan: It is not only in China... } Mentions: {'China':'Settlement','Nigeria':'Settlement','Dog':'Animal'}\n",
            "56 Tweet: {@gayinjun horse.... } Mentions: {'Horse':'Animal'}\n",
            "57 Tweet: {RT @YunalunaArts: Did a quick Gijinka of... } Mentions: {'Squirrel':'Animal'}\n",
            "58 Tweet: {@nyahbruh me: aaa what a beautiful day. ... } Mentions: {'Squirrel':'Animal','Bird':'Animal'}\n",
            "59 Tweet: {RT @TheSun: Little squirrel finds cover ... } Mentions: {'Squirrel':'Animal','Bird':'Animal'}\n",
            "60 Tweet: {RT @benshephard: IM SAVING YOU!! 😳🤣 the ... } Mentions: {'Squirrel':'Animal'}\n",
            "61 Tweet: {Paperbacks available on Amazon worldwide... } Mentions: {'Amazon':'Company','Kindle':'Product'}\n",
            "62 Tweet: {RT @verge: Amazon to stream Premier Leag... } Mentions: {'Amazon':'Company','Premier_League':'SportsEvent'}\n",
            "63 Tweet: {Remember #newyork when itâ€™s time to vo... } Mentions: {'Amazon':'Company','New_York':'Settlement'}\n",
            "64 Tweet: {RT @Nidhi: Ok. Though Harvard is a diffe... } Mentions: {'Google':'Company'}\n",
            "65 Tweet: {@anime__irl Pressing Google in search in... } Mentions: {'Google':'Company'}\n",
            "66 Tweet: {RT @businessinsider: Researchers uncover... } Mentions: {'Google':'Company','Google_Chrome':'Product'}\n",
            "67 Tweet: {@PlayStation Shame on you Sony. 5,000 x ... } Mentions: {'PlayStation':'Product','Sony':'Company', 'Microsoft':'Company'}\n",
            "68 Tweet: {RT @bit_dam: Active #malspam #phishing o... } Mentions: {'Microsoft':'Company'}\n",
            "69 Tweet: {@RoyalScrump Okay why do it has to be so... } Mentions: {'Sony':'Company'}\n",
            "70 Tweet: {Now this action will help to buy SONY. M... } Mentions: {'Sony':'Company'}\n",
            "71 Tweet: {Mount Yamnuska, as seen from the trans C... } Mentions: {'Canada_highway_#1':'RouteOfTransportation'}\n",
            "72 Tweet: {@ProtectPubs @Fullers I used to manage T... } Mentions: {'Hammersmith Bridge':'RouteOfTransportation'}\n",
            "73 Tweet: {@richardwyatt @bathnes @CllrDineRomero @... } Mentions: {'Fullers Bridge':'RouteOfTransportation'}\n",
            "74 Tweet: {@AklTransport what plans do you have? Fu... } Mentions: {'Fullers Bridge':'RouteOfTransportation'}\n",
            "75 Tweet: {RT @Xavi_Bros: National Geographic offer... } Mentions: {'National_Geographic':'PeriodicalLiterature', 'Orion_Nebula_M42':'CelestialBody', 'NASA':'Organisation'}\n",
            "76 Tweet: {RT @Nick4125: @Parlez_me_nTory Can't wai... } Mentions: {'National_Geographic':'PeriodicalLiterature'}\n",
            "77 Tweet: {Enter to win a Summer Fun, Facts and Fan... } Mentions: {'National_Geographic':'PeriodicalLiterature'}\n",
            "78 Tweet: {RT @IamQuestioning: This week I got mail... } Mentions: {'Alumni_Magazine':'PeriodicalLiterature'}\n",
            "79 Tweet: {Magazine fans of Greater Manchester. Get... } Mentions: {'Greater_Manchester_Magazines':'PeriodicalLiterature'}\n",
            "80 Tweet: {Got my new Game Informer magazine in the... } Mentions: {'Game_Informer_Magazine':'PeriodicalLiterature'}\n",
            "81 Tweet: {@GendoWasRight @GamingEthos That's why I... } Mentions: {'Game_Informer_Magazine':'PeriodicalLiterature', 'Nintendo_Power':'PeriodicalLiterature'}\n",
            "82 Tweet: {the great-granddaughter of Game Informer... } Mentions: {'Game_Informer_Magazine':'PeriodicalLiterature', 'Harper_Magazine':'PeriodicalLiterature'}\n",
            "83 Tweet: {RT @StillTheChent: Oddly, this episode o... } Mentions: {'Alberto Tomba':'WinterSportPlayer','Katarina_Witt':'WinterSportPlayer'}\n",
            "84 Tweet: {RT @teamtipplecurls: This is awesome 👏. ... } Mentions: {'Sandra_Schmirler':'WinterSportPlayer'}\n",
            "85 Tweet: {Two BC Curlers Awarded Sandra Schmirler ... } Mentions: {'Sandra_Schmirler':'WinterSportPlayer'} \n",
            "86 Tweet: {RT @Muskoka411: Sandra Schmirler Foundat... } Mentions: {'Sandra_Schmirler':'WinterSportPlayer'}\n",
            "87 Tweet: {Sandra Schmirler Foundation Supports Kid... } Mentions: {'Sandra_Schmirler':'WinterSportPlayer'}\n",
            "88 Tweet: {Piper Eve Muirhead is also an accomplish... } Mentions: {'Eve_Muirhead':'WinterSportPlayer'}\n",
            "89 Tweet: {@MacZidane @Gary_Cridland Bremner? Willi... } Mentions: {'Eve_Muirhead':'WinterSportPlayer','Willie_Carson':'WinterSportPlayer','David_Goodwillie':'WinterSportPlayer'}\n",
            "90 Tweet: {EVE MUIRHEAD: Ryder Cup stars need to fo... } Mentions: {'Eve_Muirhead':'WinterSportPlayer','Ryder_Cup':'SportsEvent'}\n",
            "91 Tweet: {@DannyHackett @DanJohnConnolly @Ed_7991 ... } Mentions: {'Eve_Muirhead':'WinterSportPlayer','England':'Settlement','Scotland':'Settlement'}\n",
            "92 Tweet: {Movie Quotes: \"Some people can?t believe... } Mentions: {'Robin_Williams': 'Artist', 'Good_Will_Hunting': 'MusicalWork'}\n",
            "93 Tweet: {“I literally saw like a light, you know ... } Mentions: {'Charlie_Murphy': 'Artist', 'Rick_James': 'Artist', 'Dave_Chapelle': 'Artist', 'Len_Bias': 'Athlete'}\n",
            "94 Tweet: {@andres3mil @HNIC_Curtis My Dad was the ... } Mentions: {'India': 'Settlement', 'Artist': 'Settlement', 'Eddie_Murphy': 'Artist'}\n",
            "95 Tweet: {RT @JusticeForJody: @55true4u Wasn't the... } Mentions: {'Dwayne_Johnson': 'Artist', 'Earthquake': 'NaturalEvent', 'San_Andreas_(film)': 'MusicalWork'}\n",
            "96 Tweet: {RT @HWarlow: I especially like this one.... } Mentions: {'Vincent_van_Gogh': 'Artist'}\n",
            "97 Tweet: {[11:05 AM] Simplicity is the ultimate so... } Mentions: {'Leonardo_da_Vinci': 'Artist'}\n",
            "98 Tweet: {Andy Warhol And Picasso Paintings Are Cu... } Mentions: {'Andy_Warhol': 'Artist', 'Pablo_Picasso': 'Artist', 'World':'NaturalPlace'}\n",
            "99 Tweet: {@ElizabethSchir7 @CourtauldGall This was... } Mentions: {'Claude_Monet': 'Artist'}\n",
            "100 Tweet: {RT @factflex: The Incredible Hulk’s gree... } Mentions: {'Hulk': 'ComicsCharacter', 'Stan_Lee': 'Artist'}\n",
            "101 Tweet: {This world doesn't even feel real anymor... } Mentions: {'Frank_Miller_(comics)': 'Artist'}\n",
            "102 Tweet: {The true partner of man is absolute God.... } Mentions: {'Rampal':'Cleric'}\n",
            "103 Tweet: {RT @Allah Kabir_: Must Watch Spiritual D... } Mentions: {'Kabir':'Cleric', 'Rampal':'Cleric'}\n",
            "104 Tweet: {RT @Rabbusaur1: World needs true worship... } Mentions: {'Rampal':'Cleric', 'World':'NaturalPlace'}\n",
            "105 Tweet: {Today’s thought for the day is from Pope... } Mentions: {'Pope_Francis':'Pope'}\n",
            "106 Tweet: {RT @Pontifex: In the Eucharist, Jesus dr... } Mentions: {'Pope_Francis':'Cleric'}\n",
            "107 Tweet: {The inauguration of the St. John Paul II... } Mentions: {'Pope_John_Paul_II':'Pope'}\n",
            "108 Tweet: {RT @CatholicBishops: Today’s thought for... } Mentions: {'Pope_Francis':'Pope'}\n",
            "109 Tweet: {RT @TheDailyShow: Nobody understands law... } Mentions: {'Fox_News':'Broadcaster'}\n",
            "110 Tweet: {@revrrlewis @Troy2k Fox news turns on th... } Mentions: {'Fox_News':'Broadcaster', 'Donald_Trump':'Politician'}\n",
            "111 Tweet: {@ABC @MarthaRaddatz F this guy. I’m sure... } Mentions: {'ABC_News':'Broadcaster','Democratic_Party_(United_States)':'Organisation','America':'Settlement'}\n",
            "112 Tweet: {RT @therealfrfr: @Cernovich Well, it’s c... } Mentions: {'ABC_News':'Broadcaster','NBC_News':'Broadcaster','Fox_News':'Broadcaster','John_Bolton':'Politician'}\n",
            "113 Tweet: {RT @of_kurth: The Daytime Emmys will be ... } Mentions: {'Emmy_Award':'SocietalEvent','CBS_News':'Broadcaster'}\n",
            "114 Tweet: {RT @JacksonWang852: Hey LA! I'm joining ... } Mentions: {'Los_Angeles':'Settlement','KIIS-FM':'Broadcaster'}\n",
            "115 Tweet: {RT @1027KIISFM: Tell us why your dad is ... } Mentions: {'KIIS-FM':'Broadcaster','Fathers_Day':'SocietalEvent'}\n",
            "116 Tweet: {RT @michaelbeatty3: That Seattle thing? ... } Mentions: {'Seattle':'Settlement', 'Fox_News':'Breoadcaster','Washington':'Settlement'}\n",
            "117 Tweet: {RT @ABC: NEW: 'I don't think he's fit fo... } Mentions: {'ABC_News':'Broadcaster', 'John_Bolton':'Politician','Donald_Trump':'Politician'}\n",
            "118 Tweet: {RT @JamesMelville: The Daily Mail and Fo... } Mentions: {'Daily_Mail':'Broadcaster','Fox_News':'Broadcaster'}\n",
            "119 Tweet: {@NMFCOfficial congratulations to the Nor... } Mentions: {'North_Melbourne_Football_Club':'SportsTeam', 'Rhyce_Shaw': 'Athlete', 'Premiership': 'SportsLeague'}\n",
            "120 Tweet: {@Adelaide_FC Thankyou Rory, and its true... } Mentions: {'Adelaide_Football_Club':'SportsTeam', 'Rory_Sloane':'Athlete', 'Port_Adelaide_Football_Club':'SportsTeam', 'Crouch_Brothers': 'Athlete'}\n",
            "121 Tweet: {RT @MARCAinENGLISH: Asensio could make h... } Mentions: {'Marca_(newspaper)':'PeriodicalLiterature', 'Marco_Asensio':'Athlete', 'Real_Madrid':'SportsTeam'}\n",
            "122 Tweet: {RT @LegionHoops: 10 years ago today, the... } Mentions: {'Lakers': 'SportsTeam', 'Celtics': 'SportsTeam', 'Kobe_Bryant': 'Athlete'}\n",
            "123 Tweet: {Best Off-Season for Chicago Bulls: Firin... } Mentions: {'Chicago_Bulls': 'SportsTeam', 'Jim_Boylen': 'Athlete', 'Kenny_Atkinson': 'Athlete', 'Otto_Porter': 'Athlete', 'Tomas_Satoransky': 'Athlete', 'Thaddeus_Young': 'Athlete', 'Anthony_Davis': 'Athlete'}\n",
            "124 Tweet: {Posted withregram • _riccardocarretta_ @... } Mentions: {'Cycling_Team_Friuli':'SportsTeam'}\n",
            "125 Tweet: {RT @james_baguma8: Congratulations to th... } Mentions: {'Soroti_Secondary_School':'EducationalInstitution', 'Ntare_School':'EducationalInstitution'}\n",
            "126 Tweet: {@mikemacman21 @DetroitRedWings @Avalanch... } Mentions: {'Detroit_Red_Wings':'SportsTeam'}\n",
            "127 Tweet: {RT @pauleddison: Exactly a year since To... } Mentions: {'Toulouse_Olympique':'SportsTeam', 'ASM_Clermont_Auvergne':'SportsTeam', 'Thomas_Ramos':'Athlete'}\n",
            "128 Tweet: {RT @cricketleinster: Happy Birthday to A... } Mentions: {'Leinster_Cricket_Union':'SportsTeam', 'Fingal': 'Settlement'}\n",
            "129 Tweet: {1976 KHSAA BOYS STATE CHAMPIONS. Mammoth... } Mentions: {'Kentucky_High_School_Athletic_Association':'EducationalInstitution', 'Mammoth_Cave_National_Park': 'NaturalPlace'}\n",
            "130 Tweet: {… As leading members of the Internationa... } Mentions: {'Thwaites_Glacier': 'NaturalPlace', 'Antarctic': 'NaturalPlace'}\n",
            "131 Tweet: {@CcibChris I saw them flying low in trai... } Mentions: {'Aletsch_Glacier': 'NaturalPlace', 'Rhône':'NaturalPlace'}\n",
            "132 Tweet: {“Never give up. Quitting is not an optio... } Mentions: {'Mount_Everest':'NaturalPlace'}\n",
            "133 Tweet: {@shygiirI can't wait to see how they pla... } Mentions: {'Mountain_range':'NaturalPlace'}\n",
            "134 Tweet: {RT @corey_kaye: Caught the night boat to... } Mentions: {'Catania': 'Settlement', 'Sicily': 'Settlement', 'Mount_Etna':'NaturalPlace'}\n",
            "135 Tweet: {Krakatoa volcanic eruption is the loudes... } Mentions: {'Krakatoa': 'NaturalPlace', 'Volcanic_Eruption':'NaturalEvent'}\n",
            "136 Tweet: {RT @DaleALeckie: Did you know … that a c... } Mentions: {'Volcanic_Eruption':'NaturalEvent'}\n",
            "137 Tweet: {RT @KesariDhwaj: Karakorum Highway (blue... } Mentions: {'Karakoram_Highway':'NaturalPlace','Mountain_range':'NaturalPlace', 'Khunjerab_Pass':'NaturalPlace'}\n",
            "138 Tweet: {RT @Harvard: Two professors at the @Kenn... } Mentions: {'Harvard':'EducationalInstitution', 'Kennedy_School':'EducationalInstitution'}\n",
            "139 Tweet: {@chronicle Incredibly well said! I’m sta... } Mentions: {'Harvard':'EducationalInstitution'}\n",
            "140 Tweet: {Pi is a new cryptocurrency that you can ... } Mentions: {'Stanford':'EducationalInstitution'}\n",
            "141 Tweet: {Yall if I have to see 'two MIT graduates... } Mentions: {'Massachusetts_Institute_of_Technology':'EducationalInstitution'}\n",
            "142 Tweet: {RT @mitsmr: Morela Hernandez, associate ... } Mentions: {'University_of_Virginia':'EducationalInstitution'}\n",
            "143 Tweet: {US blacklists 'China's MIT' as tech war ... } Mentions: {'United_States':'Settlement','China':'Settlement','Massachusetts_Institute_of_Technology':'EducationalInstitution'}\n",
            "144 Tweet: {RT @BBCPropaganda: I have no problem wit... } Mentions: {'Oxford':'EducationalInstitution'}\n",
            "145 Tweet: {RT @Sanjay_Dixit: Oxford University told... } Mentions: {'Oxford':'EducationalInstitution'}\n",
            "146 Tweet: {@Belcourtoi Who made the 3000 tests? CIG... } Mentions: {'Oxford':'EducationalInstitution'}\n",
            "147 Tweet: {RT @Helper37216548: Do you know that, Ha... } Mentions: {'Harvard':'EducationalInstitution'}\n",
            "148 Tweet: {RT @brfootball: Timo Werner will join Ch... } Mentions: {'Timo_Werner':'Athlete','Chelsea_FC':'SportsTeam','Champions_League':'Tournament'}\n",
            "149 Tweet: {Arsenal played some good football yester... } Mentions: {'Arsenal':'SportsTeam','Champions_League':'Tournament'}\n",
            "150 Tweet: {RT @AccelerateTV: Champions league to be... } Mentions: {'Champions_League':'Tournament','Lisbon':'Settlement'}\n",
            "151 Tweet: {Landed 19x Neymar on ME for champions le... } Mentions: {'Neymar':'Athlete', 'Champions_League':'Tournament', 'Ligue_1':'Tournament'}\n",
            "152 Tweet: {Bianca Andreescu joins Serena Williams i... } Mentions: {'Bianca_Andreescu':'Athlete', 'Serena_Williams':'Athlete', 'US_Open':'Tournament'}\n",
            "153 Tweet: {@LarryBoorstein @renato_mariotti Gentlem... } Mentions: {'Renato_Mariotti':'Person','United_States':'Settlement','Ryder_Cup':'Tournament'}\n",
            "154 Tweet: {@mkenne10 @alexelliottgolf Being world n... } Mentions: {'Ryder_Cup':'Tournament','Sky_Sports':'TelevisionStation','Paul_McGinley':'Athlete'}\n",
            "155 Tweet: {.@andy_murray and @p2hugz doubles at Wim... } Mentions: {'Andy_Murray':'Athlete','Wimbledon':'Tournament'}\n",
            "156 Tweet: {Our latest @AFCWAcademy coaching recruit... } Mentions: {'Wimbledon':'Tournament'}\n",
            "157 Tweet: {@GaryLineker Its like your World Cup 199... } Mentions: {'Gary_Lineker':'Presenter','World_cup':'Tournament'}\n",
            "158 Tweet: {Bayley &amp; Sasha, Randy Orton are by f... } Mentions: {'WWE':'SportsEvent','Randy_Orton':'Wrestler','Bayley':'Wrestler'}\n",
            "159 Tweet: {FOOTBALL MUST FOOT THE BILL FOR ITS OWN ... } Mentions: {'Football_Match':'SportsEvent'}\n",
            "160 Tweet: {@BakedHolloway It's MMA and wrestling is... } Mentions: {'MMA':'SportsEvent','Wrestling':'SportsEvent'}\n",
            "161 Tweet: {Whohooo #ShelbyKoren has signed for #Inv... } Mentions: {'Invicta_Fighting_Championships':'SportsEvent','Kansas City':'Settlement'}\n",
            "162 Tweet: {MMA/UFC News Daily - 18 June - More news... } Mentions: {'MMA':'SportsEvent','UFC':'SportsEvent','Dana_White':'Presenter','Francis_Ngannou':'Athlete'}\n",
            "163 Tweet: {Canadian Grand Prix, Montreal, 18 June 1... } Mentions: {'Canadian_Grand_Prix':'SportsEvent','Montreal':'Settlement','Alain_Prost':'RacingDriver','Motorsport':'SportsEvent','Formula_One':'Tournament'}\n",
            "164 Tweet: {Awaited for Upcoming Grand Prix Races, @... } Mentions: {'Grand_Prix':'SportsEvent','MotoGP':'Tournament'}\n",
            "165 Tweet: {Current plan of hosting two races for #B... } Mentions: {'BahrainGP':'Race', 'Formula_One':'Tournament', 'Motorsport':'SportsEvent','Ross_Brawn':'Person'}\n",
            "166 Tweet: {@danielsamuel24 @Yrga19 No one is agains... } Mentions: {'Dam':'Infrastructure'}\n",
            "167 Tweet: {RT @Nwandering_tv: Haneda Airport Intern... } Mentions: {'Haneda_Airport':'Infrastructure'}\n",
            "168 Tweet: {I once saw a white woman go absolutely b... } Mentions: {'Airport':'Infrastructure'}\n",
            "169 Tweet: {RT @Goodable: After more than eight year... } Mentions: {'Transportation_Security_Administration':'Organisation','Airport':'Infrastructure'}\n",
            "170 Tweet: {RT @HGWilson4: Has anybody ever seen any... } Mentions: {'Liverpool_John_Lennon_Airport':'Infrastructure'}\n",
            "171 Tweet: {Moon wants to build the Digital New Deal... } Mentions: {'Hoover Dam':'Infrastructure','Korea_JoongAng_Daily':'PeriodicalLiterature'}\n",
            "172 Tweet: {@Max_Damage01 I heard they're gonna repl... } Mentions: {'Hoover Dam':'Infrastructure'}\n",
            "173 Tweet: {RT @jinspasta: do you ever just stop for... } Mentions: {'Airport':'Infrastructure'}\n",
            "174 Tweet: {@FAAN_Official This country is funny, do... } Mentions: {'Munich_Airport':'Infrastructure'}\n",
            "175 Tweet: {Bombardier to build new ‘super hangar’ a... } Mentions: {'London_Biggin_Hill_Airport':'Infrastructure'}\n",
            "176 Tweet: {RT @romanhistory1: Old Postcards of Rome... } Mentions: {'Rome':'Settlement'}\n",
            "177 Tweet: {RT @BhupenKBorah: Visited the house of S... } Mentions: {'Bihpuria':'Settlement'}\n",
            "178 Tweet: {Anyway if you're in London and in need o... } Mentions: {'London':'Settlement'}\n",
            "179 Tweet: {RT @NaikRooh: Salary of a teacher in Kar... } Mentions: {'Karachi_American_School':'EducationalInstitution','London':'Settlement'}\n",
            "180 Tweet: {RT @MikeCarlton01: Interviewed the late ... } Mentions: {'Vera_Lynn':'MusicalArtist','London':'Settlement'}\n",
            "181 Tweet: {rainy day in london https://t.co/kLhKS1O... } Mentions: {'London':'Settlement'}\n",
            "182 Tweet: {@Ben_Scallan My wife is from Carlow.  I ... } Mentions: {'Carlow':'Settlement','Leighlinbridge':'Settlement'}\n",
            "183 Tweet: {RT @hflilas: to be living in a small tow... } Mentions: {'France':'Settlement'}\n",
            "184 Tweet: {@StarShinobi Lol! That was my reaction w... } Mentions: {'Google_Maps':'Software'}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}